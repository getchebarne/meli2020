{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import gc\n",
    "import math\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def jl_to_list(fname):\n",
    "    output = []\n",
    "    with gzip.open(fname, \"rb\") as f:\n",
    "        for line in f:\n",
    "            output.append(json.loads(line))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load user data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = None\n",
    "test_size = .2\n",
    "rows = jl_to_list(\"../data/train_dataset.jl.gz\")\n",
    "if samples:\n",
    "    rows = rows[:samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 330530.\n",
      "Test data: 82633.\n"
     ]
    }
   ],
   "source": [
    "val_size = .15\n",
    "rows_train, rows_test = train_test_split(rows, test_size = test_size, random_state = 42)\n",
    "print(f\"Train data: {len(rows_train)}.\")\n",
    "print(f\"Test data: {len(rows_test)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_data = jl_to_list(\"../data/item_data.jl.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save item title strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15bccd1aadc04a369196ec7029671f35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2102277.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Max tokens: 15.\n",
      "Se Vende Hotel Boutique De 5 Estrellas, Con 37 Suites Con Vistas Al Mar Caribe\n",
      "Number of unique words:  623405\n"
     ]
    }
   ],
   "source": [
    "file = open(\"titles.txt\", \"wt\")\n",
    "max_tok = 0\n",
    "max_title = None\n",
    "tokens_hist = []\n",
    "words_ = []\n",
    "word_counter = Counter()\n",
    "for item in tqdm(item_data):\n",
    "    title = item[\"title\"]\n",
    "    words = list(title.lower().split())\n",
    "    words_ += words\n",
    "    for w in words:\n",
    "        word_counter[w] += 1\n",
    "    tokens = len(words)\n",
    "    tokens_hist.append(tokens)\n",
    "    if tokens > max_tok and tokens <= 15:\n",
    "        max_tok = tokens\n",
    "        max_title = title\n",
    "    file.write(title + \"\\n\")\n",
    "    \n",
    "print(f\"Max tokens: {max_tok}.\")\n",
    "print(max_title)\n",
    "print(\"Number of unique words: \", len(set(words_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save user query strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "673c58d7292c4ab186d7b55308141584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=330530.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for row in tqdm(rows_train):\n",
    "    queries = [item[\"event_info\"] for item in row[\"user_history\"]\n",
    "                  if item[\"event_type\"] == \"search\"]\n",
    "    \n",
    "    for q in queries:\n",
    "        file.write(q + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_word(word_counter, thresh = .7):\n",
    "    s = sum([v for v in word_counter.values()])\n",
    "    aux = word_counter.most_common()\n",
    "    i = 0\n",
    "    buf = 0\n",
    "    while buf < thresh*s:\n",
    "        buf += aux[i][1]\n",
    "        i += 1\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3443"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crop_word(word_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 900x540 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (12.5, 7.5))\n",
    "plt.xticks(np.arange(20))\n",
    "plt.hist(tokens_hist, bins = 20, density = True)\n",
    "plt.grid(\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Dot, Embedding, Flatten, GlobalAveragePooling1D, Reshape\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42 \n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
    "# (int-encoded sentences) based on window size, number of negative samples\n",
    "# and vocabulary size.\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "  # Elements of each training example are appended to these lists.\n",
    "    targets, contexts, labels = [], [], []\n",
    "\n",
    "    # Build the sampling table for vocab_size tokens.\n",
    "    sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "    # Iterate over all sequences (sentences) in dataset.\n",
    "    for sequence in tqdm.tqdm(sequences):\n",
    "        positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "              sequence, \n",
    "              vocabulary_size=vocab_size,\n",
    "              sampling_table=sampling_table,\n",
    "              window_size=window_size,\n",
    "              negative_samples=0)\n",
    "\n",
    "    # Iterate over each positive skip-gram pair to produce training examples \n",
    "    # with positive context word and negative samples.\n",
    "        for target_word, context_word in positive_skip_grams:\n",
    "            context_class = tf.expand_dims(\n",
    "              tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "            negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "              true_classes=context_class,\n",
    "              num_true=1, \n",
    "              num_sampled=num_ns, \n",
    "              unique=True, \n",
    "              range_max=vocab_size, \n",
    "              seed=SEED, \n",
    "              name=\"negative_sampling\")\n",
    "\n",
    "          # Build context and label vectors (for one target word)\n",
    "            negative_sampling_candidates = tf.expand_dims(\n",
    "              negative_sampling_candidates, 1)\n",
    "\n",
    "            context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
    "            label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "            # Append each element from the training example to global lists.\n",
    "            targets.append(target_word)\n",
    "            contexts.append(context)\n",
    "            labels.append(label)\n",
    "\n",
    "    return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ds = tf.data.TextLineDataset(\"titles.txt\").filter(lambda x: tf.cast(tf.strings.length(x), bool))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize sentences from the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a custom standardization function to lowercase the text and \n",
    "# remove punctuation.\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    return tf.strings.regex_replace(lowercase,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "# Define the vocabulary size and number of words in a sequence.\n",
    "vocab_size = 8192 + 4096\n",
    "sequence_length = 12\n",
    "\n",
    "# Use the text vectorization layer to normalize, split, and map strings to\n",
    "# integers. Set output_sequence_length length to pad all samples to same length.\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer.adapt(text_ds.batch(1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'de', 'kit', 'para', 'tenis', 'com', 'infantil', 'e', 'feminino', 'masculino', '2', 'celular', '4', 'original', 'xiaomi', 'led', '3', 'samsung', '10']\n"
     ]
    }
   ],
   "source": [
    "# Save the created vocabulary for reference.\n",
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "print(inverse_vocab[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return tf.squeeze(vectorize_layer(text))\n",
    "\n",
    "# Vectorize the data in text_ds.\n",
    "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6967053\n"
     ]
    }
   ],
   "source": [
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 184 2766  393 3726  123 2280    1 2248 4092    2    1    0] => ['casa', 'sola', 'en', 'venta', 'con', 'gran', '[UNK]', 'solo', 'pago', 'de', '[UNK]', '']\n",
      "[ 4692  4227 11247  3426   462   564   393     1     0     0     0     0] => ['resident', 'evil', 'origins', 'collection', 'nintendo', 'switch', 'en', '[UNK]', '', '', '', '']\n",
      "[4787    2    1  880  638    0    0    0    0    0    0    0] => ['falda', 'de', '[UNK]', 'piel', 'negra', '', '', '', '', '', '', '']\n",
      "[    1   578 10402  4455  2620  5897   618  7807     0     0     0     0] => ['[UNK]', 'red', 'devil', 'radeon', 'rx', '580', '8gb', 'gddr5', '', '', '', '']\n",
      "[ 672  291    1  567 2036  123 8657    1 1573   25    0    0] => ['laptop', 'hp', '[UNK]', 'core', 'duo', 'con', 'puerto', '[UNK]', 'windows', '7', '', '']\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences[:5]:\n",
    "    print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6967053/6967053 [29:30<00:00, 3934.66it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20606275 20606275 20606275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_ns = 4\n",
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=sequences, \n",
    "    window_size=2, \n",
    "    num_ns=num_ns, \n",
    "    vocab_size=vocab_size, \n",
    "    seed=SEED)\n",
    "\n",
    "print(len(targets), len(contexts), len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train - test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6052252\n",
      "2017418\n"
     ]
    }
   ],
   "source": [
    "N = len(targets)\n",
    "val_size = .25\n",
    "train_indxs, val_indxs = train_test_split(np.arange(N).astype(np.int), test_size = val_size)\n",
    "print(train_indxs.size)\n",
    "print(val_indxs.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: (((1024,), (1024, 6, 1)), (1024, 6)), types: ((tf.int32, tf.int64), tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "targets_train = [targets[i] for i in train_indxs]\n",
    "contexts_train = [contexts[i] for i in train_indxs]\n",
    "labels_train = [labels[i] for i in train_indxs]\n",
    "\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices(((targets_train, contexts_train), labels_train))\n",
    "dataset_train = dataset_train.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "dataset_train = dataset_train.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "print(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: (((1024,), (1024, 6, 1)), (1024, 6)), types: ((tf.int32, tf.int64), tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "targets_val = [targets[i] for i in val_indxs]\n",
    "contexts_val = [contexts[i] for i in val_indxs]\n",
    "labels_val = [labels[i] for i in val_indxs]\n",
    "\n",
    "dataset_val = tf.data.Dataset.from_tensor_slices(((targets_val, contexts_val), labels_val))\n",
    "dataset_val = dataset_val.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "dataset_val = dataset_val.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "print(dataset_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whole dataset (for final training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: (((1024,), (1024, 5, 1)), (1024, 5)), types: ((tf.int32, tf.int64), tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: (((1024,), (1024, 5, 1)), (1024, 5)), types: ((tf.int32, tf.int64), tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_ns):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.target_embedding = Embedding(vocab_size, \n",
    "                                          embedding_dim,\n",
    "                                          input_length=1,\n",
    "                                          name=\"w2v_embedding\", )\n",
    "        self.context_embedding = Embedding(vocab_size, \n",
    "                                           embedding_dim, \n",
    "                                           input_length=num_ns+1)\n",
    "        self.dots = Dot(axes=(3,2))\n",
    "        self.flatten = Flatten()\n",
    "        #sanity check\n",
    "        print(\"num_ns: \", num_ns)\n",
    "        print(\"embedding dim: \", embedding_dim)\n",
    "        print(\"vocab_size: \", vocab_size)\n",
    "\n",
    "    def call(self, pair):\n",
    "        target, context = pair\n",
    "        we = self.target_embedding(target)\n",
    "        ce = self.context_embedding(context)\n",
    "        dots = self.dots([ce, we])\n",
    "        return self.flatten(dots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_ns:  4\n",
      "embedding dim:  16\n",
      "vocab_size:  12288\n",
      "Epoch 1/15\n",
      "20123/20123 [==============================] - 78s 4ms/step - loss: 0.9455 - accuracy: 0.6308\n",
      "Epoch 2/15\n",
      "20123/20123 [==============================] - 78s 4ms/step - loss: 0.7563 - accuracy: 0.7158\n",
      "Epoch 3/15\n",
      "20123/20123 [==============================] - 78s 4ms/step - loss: 0.7270 - accuracy: 0.7273\n",
      "Epoch 4/15\n",
      "20123/20123 [==============================] - 78s 4ms/step - loss: 0.7170 - accuracy: 0.7312\n",
      "Epoch 5/15\n",
      "20123/20123 [==============================] - 78s 4ms/step - loss: 0.7119 - accuracy: 0.7331\n",
      "Epoch 6/15\n",
      "20123/20123 [==============================] - 78s 4ms/step - loss: 0.7085 - accuracy: 0.7344\n",
      "Epoch 7/15\n",
      "20123/20123 [==============================] - 78s 4ms/step - loss: 0.7062 - accuracy: 0.7353\n",
      "Epoch 8/15\n",
      "20123/20123 [==============================] - 78s 4ms/step - loss: 0.7044 - accuracy: 0.7359\n",
      "Epoch 9/15\n",
      "20123/20123 [==============================] - 78s 4ms/step - loss: 0.7030 - accuracy: 0.7364\n",
      "Epoch 10/15\n",
      "20123/20123 [==============================] - 78s 4ms/step - loss: 0.7019 - accuracy: 0.7368\n",
      "Epoch 11/15\n",
      "20123/20123 [==============================] - 78s 4ms/step - loss: 0.7010 - accuracy: 0.7372\n",
      "Epoch 12/15\n",
      "20123/20123 [==============================] - 78s 4ms/step - loss: 0.7002 - accuracy: 0.7374\n",
      "Epoch 13/15\n",
      "20123/20123 [==============================] - 78s 4ms/step - loss: 0.6996 - accuracy: 0.7377\n",
      "Epoch 14/15\n",
      "20123/20123 [==============================] - 78s 4ms/step - loss: 0.6990 - accuracy: 0.7379\n",
      "Epoch 15/15\n",
      "20123/20123 [==============================] - 77s 4ms/step - loss: 0.6985 - accuracy: 0.7380\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "embedding_dim = 16\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim, num_ns)\n",
    "word2vec.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "h = word2vec.fit(dataset, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_v = io.open('vectors_q_16.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta_q_16.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "    if  index == 0: continue # skip 0, it's padding.\n",
    "    vec = weights[index] \n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 203M\r\n",
      "-rw-rw-r--. 1 guillermo.etchebarne guillermo.etchebarne   1M Nov 25 20:05 meta_q.tsv\r\n",
      "-rw-rw-r--. 1 guillermo.etchebarne guillermo.etchebarne   1M Nov 24 14:58 meta_tit_32.tsv\r\n",
      "-rw-rw-r--. 1 guillermo.etchebarne guillermo.etchebarne 197M Nov 25 18:38 titles.txt\r\n",
      "-rw-rw-r--. 1 guillermo.etchebarne guillermo.etchebarne   5M Nov 25 20:05 vectors_q.tsv\r\n",
      "-rw-rw-r--. 1 guillermo.etchebarne guillermo.etchebarne   2M Nov 24 14:58 vectors_tit_32.tsv\r\n",
      "-rw-r--r--. 1 guillermo.etchebarne guillermo.etchebarne   1M Nov 23 18:47 word2vec.ipynb\r\n",
      "-rw-rw-r--. 1 guillermo.etchebarne guillermo.etchebarne   1M Nov 25 20:04 word2vec_items.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l --block-size=M"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
